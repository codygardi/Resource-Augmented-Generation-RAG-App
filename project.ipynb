{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873e67fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Project Overview**\n",
    "\n",
    "You’re building your own **AI-powered knowledge base chatbot** that can answer questions using your personal or company data — things like PDFs, markdown files, and notes.\n",
    "The model itself runs **locally** using **LM Studio**, and your **Streamlit** front-end provides a simple web chat interface.\n",
    "Behind the scenes, the system uses **Retrieval-Augmented Generation (RAG)** — meaning it first searches your documents for relevant text, then feeds that into a local LLM to craft an intelligent, grounded response.\n",
    "\n",
    "All free. All offline. All under your control.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Roadmap**\n",
    "\n",
    "**1. Environment Setup**\n",
    "\n",
    "* Install **Python 3.10.10** and **CUDA 11.8**-compatible drivers.\n",
    "* Create a new folder `rag-chatbot/` with subfolders: `app/`, `data/`, `models/`, `scripts/`.\n",
    "* Add a `requirements.txt` file (from the previous step).\n",
    "* Set up a virtual environment and install dependencies.\n",
    "\n",
    "**2. LM Studio Configuration**\n",
    "\n",
    "* Install LM Studio (open-source desktop app).\n",
    "* Download a small local model (e.g., `Mistral-7B-Instruct` or `Llama-3-Instruct`).\n",
    "* Enable the **local server API** inside LM Studio and note the port (usually `http://localhost:1234`).\n",
    "\n",
    "**3. Data Ingestion + Embeddings**\n",
    "\n",
    "* Place PDFs or markdowns inside `data/`.\n",
    "* Use a `scripts/ingest_data.py` script to:\n",
    "\n",
    "  * Read documents with `pypdf` or plain text loaders.\n",
    "  * Generate embeddings using `sentence-transformers`.\n",
    "  * Store them in a local **FAISS** vector index.\n",
    "\n",
    "**4. RAG Pipeline Assembly**\n",
    "\n",
    "* Build a retrieval pipeline with **LangChain**:\n",
    "\n",
    "  * Use FAISS to search for the most relevant text chunks.\n",
    "  * Send the top-ranked chunks and user question to LM Studio via HTTP.\n",
    "  * Combine the retrieved info + model output for final answers.\n",
    "\n",
    "**5. Streamlit Front-End**\n",
    "\n",
    "* Create `app/app.py`:\n",
    "\n",
    "  * Include a text input for user queries.\n",
    "  * Display chat history (user + bot messages).\n",
    "  * Connect to your RAG backend functions to fetch responses live.\n",
    "\n",
    "**6. Testing and Tuning**\n",
    "\n",
    "* Run `streamlit run app/app.py`.\n",
    "* Ask domain questions — confirm that the chatbot uses your documents, not just general knowledge.\n",
    "* Adjust chunk size, embedding model, or retrieval parameters for accuracy.\n",
    "\n",
    "**7. Deployment (Optional)**\n",
    "\n",
    "* Package as a standalone desktop app with **Streamlit** or run in a private LAN.\n",
    "* Everything stays **offline and local**, respecting data privacy.\n",
    "\n",
    "---\n",
    "\n",
    "rag-chatbot/\n",
    "\n",
    "├── app/               # Streamlit + RAG logic will go here later\n",
    "\n",
    "├── data/              # Your PDFs / markdown / knowledge base files\n",
    "\n",
    "├── models/            # (optional) local embeddings/models you might download\n",
    "\n",
    "├── scripts/           # helper scripts we’ll write later\n",
    "\n",
    "├── requirements.txt   # pinned versions (see below)\n",
    "\n",
    "└── .env               # for LM Studio endpoint and secrets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b48f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project_name': 'company_rag_chatbot',\n",
       " 'description': 'A RAG pipeline for internal knowledge base Q&A.',\n",
       " 'data_dirs': {'policies': WindowsPath('data/policies'),\n",
       "  'runbooks': WindowsPath('data/runbooks'),\n",
       "  'faq': WindowsPath('data/faq')},\n",
       " 'embedding_model': 'sentence-transformers/all-mpnet-base-v2',\n",
       " 'vector_store': 'faiss',\n",
       " 'device_preference': 'cuda'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project metadata / configuration\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_CONFIG = {\n",
    "    \"project_name\": \"company_rag_chatbot\",\n",
    "    \"description\": \"A RAG pipeline for internal knowledge base Q&A.\",\n",
    "    \"data_dirs\": {\n",
    "        \"policies\": Path(\"data/policies\"),\n",
    "        \"runbooks\": Path(\"data/runbooks\"),\n",
    "        \"faq\": Path(\"data/faq\"),\n",
    "    },\n",
    "    \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"vector_store\": \"faiss\",\n",
    "    \"device_preference\": \"cuda\",  # Will auto-detect GPU if available\n",
    "}\n",
    "PROJECT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a8c16",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports\n",
    "\n",
    "Before building the RAG pipeline, we’ll import all required libraries.  \n",
    "This section sets up the environment, verifies GPU availability, and ensures that we can use FAISS and Sentence Transformers efficiently.  \n",
    "\n",
    "If a GPU is detected, embeddings and FAISS indexing will use CUDA for faster computation.  \n",
    "Otherwise, it will automatically fall back to CPU mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP and Embedding models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector store\n",
    "import faiss\n",
    "\n",
    "# LangChain for document loading and chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "\n",
    "# Check environment\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device.upper()}\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
    "print(\"Embedding model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03016e6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
